# 『밑바닥부터 시작하는 딥러닝』

이 저장소는 『[밑바닥부터 시작하는 딥러닝](http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198)』(한빛미디어, 2017)의 책을 보고 소스코드를 따라 쳐보있습니다.  

([『밑바닥부터 시작하는 딥러닝』의 깃허브 저장소는 이곳](https://github.com/WegraLee/deep-learning-from-scratch)입니다.)

### Chapter3
> - 계단 함수의 그래프 만들기
> - 시그모이드 함수 구현하기
> - ReLU 함수 구현하기
> - 다차원 배열로 신경망 구성하기
> - Softmax 함수 구현하기
> - MNIST 데이터셋을 이용하여 손글씨 숫자 인식하기
> - 신경망의 추론 처리

### Chapter 4
> - 평균 제곱 오차 구하기 (MSE)
> - 교차 엔트로피 오차 구하기 (Cross Entropy Error) - minibatch용
> - 미니배치 학습
> - 미분 - 반올림 오차를 고려하여 미분 함수를 만들기
> - 기울기 계산
> - 경사 하강법
> - 신경망에서의 기울기

### Chapter 4_1
> - 학습 알고리즘 구현하기
> - 미니배치 학습 구현하기

### Chapter 5
> - 곱셈 계층 : 순전파, 역전파 구현하기

### Chapter 5_1
> - ReLU 클래스 구현
> - Sigmoid 클래스 구현
> - Affine 클래스 구현

### Chapter 5_2
> - MNIST 데이터셋을 이용하여 이중 신경망 학습시키기

### Chapter 6
> - SGD 구현
> - Momentum 구현
> - AdaGrad 구현
> - Adam 구현
> - MNIST 데이터셋을 이용하여 최적화 함수 비교하기

### Chapter 6_1
> - 초기값과 활성화 함수 간의 비교

### Chapter 6_2
> - MNIST 데이터 셋으로 본 가중치 초기화 방법 비교

### Chapter 6_3
> - MNIST 데이터 셋으로 본 batch normalization 방법 비교

### Chapter 6_4
> - MNIST 데이터 셋으로 본 weight decay를 이용하여 오버피팅을 기막는 방법(신경망이 단순할 때 효과적)

### Chapter 6_5
> - MNIST 데이터 셋으로 본 dropout을 이용하여 오버피팅을 막는 방법(신경망 구조가 복잡할 때 효과적)

### Chapter 6_6
> - MNIST 데이터셋으로 검증셋을 이용하여 하이퍼파라미터의 최적화 구현

### Chapter 7
> - 합성곱 계층 구현하기
> - 풀링 계층 구현하기

### Chapter 7_1
> - 단순한 합성곱 신경망 구현하기

### Chapter 7_2
> - Chapter7_1에 있는 SimpleConvNet을 이용하여 MNIST 데이터 셋 학습시키기

### Chapter 7_3
> - Chapter7_1에 있는 SimpleConvNet을 이용하여 필터 계수 시각화 하기

### Chapter 8
> - 좀 더 깊은 합성곱 신경망 구현하기

### Chapter 8_1
> - Chapter8에 있는 DeepConvNet을 이용하여 MNIST 데이터 셋 학습시키기

### Chapter 8_2
> - 수치 정밀도를 반정밀도(16비트)로 낮춰 계산하여 배정밀도(64비트)일 때와 정확도를 비교하
